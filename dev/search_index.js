var documenterSearchIndex = {"docs":
[{"location":"#NLPModelsJuMP.jl-documentation-1","page":"Home","title":"NLPModelsJuMP.jl documentation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package defines a NLPModels model using MathProgBase and JuMP.jl. This documentation is specific for this model. Please refer to the NLPModels documentation if in doubt.","category":"page"},{"location":"#Install-1","page":"Home","title":"Install","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Install NLPModelsJuMP.jl with the following commands.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"pkg> add NLPModelsJuMP","category":"page"},{"location":"#Contents-1","page":"Home","title":"Contents","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"NLPModelsJuMP is a combination of NLPModels and JuMP, as the name implies. Sometimes it may be required to refer to the specific documentation, as we'll present here only the documention specific to NLPModelsJuMP.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/#MathProgNLPModel-1","page":"Tutorial","title":"MathProgNLPModel","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"MathProgNLPModel","category":"page"},{"location":"tutorial/#NLPModelsJuMP.MathProgNLPModel","page":"Tutorial","title":"NLPModelsJuMP.MathProgNLPModel","text":"MathProgNLPModel(model, name=\"Generic\")\n\nConstruct a MathProgNLPModel from a MathProgModel.\n\n\n\n\n\nMathProgNLPModel(model; kwargs...)\n\nConstruct a MathProgNLPModel from a JuMP Model.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"MathProgNLPModel is a simple yet efficient model. It uses JuMP to define the problem, and can be accessed through the NLPModels API. An advantage of MathProgNLPModel over simpler models such as ADNLPModels is that they provide sparse derivates.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's define the famous Rosenbrock function","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"f(x) = (x_1 - 1)^2 + 100(x_2 - x_1^2)^2","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"with starting point x^0 = (-1210).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels, NLPModelsJuMP, JuMP\n\nx0 = [-1.2; 1.0]\nmodel = Model() # No solver is required\n@variable(model, x[i=1:2], start=x0[i])\n@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n\nnlp = MathProgNLPModel(model)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's get the objective function value at x^0, using only nlp.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fx = obj(nlp, nlp.meta.x0)\nprintln(\"fx = $fx\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's try the gradient and Hessian.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"gx = grad(nlp, nlp.meta.x0)\nHx = hess(nlp, nlp.meta.x0)\nprintln(\"gx = $gx\")\nprintln(\"Hx = $Hx\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice how only the lower triangle of the Hessian is stored, which is the default for NLPModels.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Let's do something a little more complex here, defining a function to try to solve this problem through steepest descent method with Armijo search. Namely, the method","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Given x^0, varepsilon  0, and eta in (01). Set k = 0;\nIf Vert nabla f(x^k) Vert  varepsilon STOP with x^* = x^k;\nCompute d^k = -nabla f(x^k);\nCompute alpha_k in (01 such that f(x^k + alpha_kd^k)  f(x^k) + alpha_keta nabla f(x^k)^Td^k\nDefine x^k+1 = x^k + alpha_kx^k\nUpdate k = k + 1 and go to step 2.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra\n\nfunction steepest(nlp; itmax=100000, eta=1e-4, eps=1e-6, sigma=0.66)\n  x = nlp.meta.x0\n  fx = obj(nlp, x)\n  ∇fx = grad(nlp, x)\n  slope = dot(∇fx, ∇fx)\n  ∇f_norm = sqrt(slope)\n  iter = 0\n  while ∇f_norm > eps && iter < itmax\n    t = 1.0\n    x_trial = x - t * ∇fx\n    f_trial = obj(nlp, x_trial)\n    while f_trial > fx - eta * t * slope\n      t *= sigma\n      x_trial = x - t * ∇fx\n      f_trial = obj(nlp, x_trial)\n    end\n    x = x_trial\n    fx = f_trial\n    ∇fx = grad(nlp, x)\n    slope = dot(∇fx, ∇fx)\n    ∇f_norm = sqrt(slope)\n    iter += 1\n  end\n  optimal = ∇f_norm <= eps\n  return x, fx, ∇f_norm, optimal, iter\nend\n\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\"x = $x\")\nprintln(\"fx = $fx\")\nprintln(\"ngx = $ngx\")\nprintln(\"optimal = $optimal\")\nprintln(\"iter = $iter\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Maybe this code is too complicated? If you're in a class you just want to show a Newton step.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"f(x) = obj(nlp, x)\ng(x) = grad(nlp, x)\nH(x) = Symmetric(hess(nlp, x), :L)\nx = nlp.meta.x0\nd = -H(x)\\g(x)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"or a few","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"for i = 1:5\n  global x\n  x = x - H(x)\\g(x)\n  println(\"x = $x\")\nend","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Notice how we can use the method with different NLPModels:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"f(x) = (x[1] - 1.0)^2 + 100 * (x[2] - 1.0)^2\n\nadnlp = ADNLPModel(f, x0)\nx, fx, ngx, optimal, iter = steepest(adnlp)","category":"page"},{"location":"tutorial/#OptimizationProblems-1","page":"Tutorial","title":"OptimizationProblems","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The package OptimizationProblems provides a collection of problems defined in JuMP format, which can be converted to MathProgNLPModel.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using OptimizationProblems # Defines a lot of JuMP models\n\nnlp = MathProgNLPModel(woods())\nx, fx, ngx, optimal, iter = steepest(nlp)\nprintln(\"fx = $fx\")\nprintln(\"ngx = $ngx\")\nprintln(\"optimal = $optimal\")\nprintln(\"iter = $iter\")","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Constrained problems can also be converted.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels, NLPModelsJuMP, JuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2] >= 0.0, start=x0[i])\n@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n@constraint(model, x[1] + x[2] == 3.0)\n@NLconstraint(model, x[1] * x[2] >= 1.0)\n\nnlp = MathProgNLPModel(model)\n\nprintln(\"cx = $(cons(nlp, nlp.meta.x0))\")\nprintln(\"Jx = $(jac(nlp, nlp.meta.x0))\")","category":"page"},{"location":"tutorial/#MathProgNLSModel-Tutorial-1","page":"Tutorial","title":"MathProgNLSModel Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"MathProgNLSModel","category":"page"},{"location":"tutorial/#NLPModelsJuMP.MathProgNLSModel","page":"Tutorial","title":"NLPModelsJuMP.MathProgNLSModel","text":"Construct a MathProgNLSModel from two MathProgModels.\n\n\n\n\n\nMathProgNLSModel(cmodel, F)\n\nConstruct a MathProgNLSModel from a JuMP Model and a vector of NLexpression.\n\n\n\n\n\n","category":"type"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"MathProgNLSModel is a model for nonlinear least squares using JuMP, The objective function of NLS problems has the form f(x) = tfrac12F(x)^2, but specialized methods handle F directly, instead of f. To use MathProgNLSModel, we define a JuMP model without the objective, and use NLexpressions to define the residual function F. For instance, the Rosenbrock function can be expressed in nonlinear least squares format by defining","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"F(x) = beginbmatrix x_1 - 1 10(x_2 - x_1^2) endbmatrix","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"and noting that f(x) = F(x)^2 (the constant frac12 is ignored as it doesn't change the solution). We implement this function as","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NLPModels, NLPModelsJuMP, JuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2], start=x0[i])\n@NLexpression(model, F1, x[1] - 1)\n@NLexpression(model, F2, 10 * (x[2] - x[1]^2))\n\nnls = MathProgNLSModel(model, [F1, F2], name=\"rosen-nls\")\n\nresidual(nls, nls.meta.x0)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"jac_residual(nls, nls.meta.x0)","category":"page"},{"location":"tutorial/#NLPtoMPB-Convert-NLP-to-MathProgBase-1","page":"Tutorial","title":"NLPtoMPB - Convert NLP to MathProgBase","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"NLPtoMPB","category":"page"},{"location":"tutorial/#NLPModelsJuMP.NLPtoMPB","page":"Tutorial","title":"NLPModelsJuMP.NLPtoMPB","text":"mp = NLPtoMPB(nlp, solver)\n\nReturn a MathProgBase model corresponding to an AbstractNLPModel.\n\nArguments\n\nnlp::AbstractNLPModel\nsolver::AbstractMathProgSolver a solver instance, e.g., IpoptSolver()\n\nCurrently, all models are treated as nonlinear models.\n\nReturn values\n\nThe function returns a MathProgBase model mpbmodel such that it should be possible to call\n\nMathProgBase.optimize!(mpbmodel)\n\n\n\n\n\n","category":"function"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In addition to creating NLPModels using JuMP, we might want to convert an NLPModel to a MathProgBase model to use the solvers available. For instance","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using Ipopt, NLPModels, NLPModelsJuMP, LinearAlgebra, JuMP, MathProgBase\n\nnlp = ADNLPModel(x -> dot(x, x), ones(2),\n                 c=x->[x[1] + 2 * x[2] - 1.0], lcon=[0.0], ucon=[0.0])\nmodel = NLPtoMPB(nlp, IpoptSolver())\n\nMathProgBase.optimize!(model)","category":"page"},{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"","category":"page"}]
}
